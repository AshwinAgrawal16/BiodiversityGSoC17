IDEAS ON HOW TO MANAGE THE DwC DATA:
After reading the research paper and reading about biodiversity data handing
from different resources.
The major problems which exist with the data are its standardization. As the
biodiversity is data is collected around the world so each recording of the data
has its own standard set.
So to rank the data according to the Darwin Core a simple key based
approached can be followed.
Darwin core basically consists of nine parts :-Occurence, Event, Location,
Identification, Taxon, GeologicalContext, Relationship, Fact, Record level
terms. All these contain massive amount of data based on each occurrence.
There are many R packages (“spocc”,”rgbif”,”finch”,etc) which retrieve the
data from portals like GBIF, ALA, ebird, BISON etc. So as there are many portals
and many packages the discrepancy in data is obvious.
To handle this discrepancy we can extract data from all these portals using the
different packages ,and then try to combine them by checking the duplicity
,validity of the data and standardisation of the data based on the Darwin
core. A uniform key based comparisons of the data from different portals can
be used (like primary key used in SQL and MongoDB to mange and clean
records from different portals) there by setting a uniform flagging system.
The above will handle the “Data retrieval and pepping part” and “Data
management “part of the project.
Next several functions to clean the data like flagging the coordinates based on
capital, centroid, seas, urban, areas, headquarters, outliers, date etc can be
checked.
(I have already developed the date-(POSIXlt and POSIXct) and centroid
function).
The diversity of the data can also be parameter to check the quality of data.
Basically there will be master database where we can insert data from small
databases after cleaning and standardizing the data based on Darwin core.
Moreover to handle situations where data is very less and fields are very large
segregated or sparse matrix can be build to save the space and increase the
efficiency of the running time.
I am in the process of building some small functions based in the above idea
which will retrieve data from different portal using packages “spocc” ,“rgbif”
and possibly “finch” , clean the data, standardize the data and then uniformly
flag it and then push it to GQ API.
Several speed optimization can be used to segregate and flag the data ,some
which I will implement and rest I am exploring from different research studies.
This whole work can be assembled as R package and proper documentation or
even a shiny app can be build.
Meanwhile I am still exploring new and efficient ways to handle the DwC data
and if I come up with something more efficient and user friendly I will let you
know.
Further if there are any other queries and any suggestions please let me know.
